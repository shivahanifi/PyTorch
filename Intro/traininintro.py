# -*- coding: utf-8 -*-
"""Trainin Intro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d6Bg2mUVKkxXKFOrhOQBQ8cdlrsVZaFC

## Training the model 
We'll be training the neural network built previously by learning how to iterate over our data, pass to the model, calculate loss from the result, and then do backpropagation to slowly fit our model to the data. The related material for this tutorial can be found in this [link](https://pythonprogramming.net/training-deep-learning-neural-network-pytorch/).
"""

import torch
import torchvision
from torchvision import transforms, datasets
import torch.nn as nn
import torch.nn.functional as F

train = datasets.MNIST('', train=True, download=True,
                       transform=transforms.Compose([
                           transforms.ToTensor()
                       ]))

test = datasets.MNIST('', train=False, download=True,
                       transform=transforms.Compose([
                           transforms.ToTensor()
                       ]))


trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)
testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 64)
        self.fc4 = nn.Linear(64, 10)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return F.log_softmax(x, dim=1)

net = Net()
print(net)

"""## Loss and Optimizer
### Loss 
It is a measurement of how far off the neural network is from the targeted output. The degree to which you're wrong doesn't matter in terms of the choice necessarily, but in terms of you learning, it does. The goal over time is to have loss decrease.

- *nn.CrossEntropyLoss()*

  This criterion computes the cross entropy loss between input and target. It is useful when training a classification problem with C classes. The input is expected to contain raw, unnormalized scores for each class.



  In general, you're going to have two types of classes. One will just be a scalar value, the other is what's called a one_hot array/vector. Depending on what your targets look like, you will need a specific loss.
  - *one_hot array/vector*:
For example, [1, 0, 0, 0, 0, 0, 0 ,0 ,0 ,0] is a one_hot array where quite literally one element only is a 1 and the rest are zero. The index that is hot is the classification.

This data is specifying a scalar class and for these scalar classifications, cross entropy is used.


### Optimizer
Its job is to go through and adjust our model's adjustable parameters  like the weights based on the loss, to slowly, over time, fit our data.  (i.e. lowers the loss). We do not optimize for accuracy, but accuracy follows the loss.

Import the ``torch.optim`` and specify the optimizer. 
- *optim.Adam(params, lr=0.001)*
Implements Adam algorithm (Adaptive Momentum).

  - *net.parameters()*
  
   refers to everything that is adjustable in our model. There might also be layers that are not adjustable (e.g. transfer learning).
  - *lr*

   Learning rate. It dictates the size of the step your optimizer will take. We are actually telling the optimizer to lower the loss but only take certain size steps, this way we do not overfit the data. There is no way to find the best learning rate usually decaying learning rate is used. Here it is set to 0.001 which also can be written as 1e-3. For more complex tasks, you will see a learning rate with what's called a decay. Basically you start the learning rate at something like 0.001, or 0.01...etc, and then over time, that learning rate gets smaller and smaller.
"""

import torch.optim as optim
loss_function= nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

"""## Iterating over the data

We gonna iterate over our data and pass it through the model. A full pass through the data is called `` epoch``. We would prefer to pass through the data several times and that is the reason we set the number of epochs.  In general, you will probably have somewhere between 3 and 10 epochs, but there's no hard rule here.
- *data*

 It is a batch of featuresets and labels. We decompose it into the batch of features ``X`` and targets ``y``.

- *net.zero_grad()*

  Sets gradients of all parameters (including the parametes of the submodels) to 0. 
  
  Once we pass data through our neural network, getting an output, we can compare that output to the desired output. With this, we can compute the gradients for each parameter, which our optimizer (Adam, SGD...etc) uses as information for updating weights.
  Every time before you pass data through your neural network you set the gradient to 0. Because if you don't 0 the gradient they will keep adding up for every pass, and then we'll be re-optimizing for previous gradients that we already optimized for. 

- *F.nll_loss(input, target, weight=None)*

  The negative log likelihood loss.

  If your dataset is a scalar value and not a vector use the ``nll`` loss. But if your data is a one-hot vector use mean square error.

- *loss.backward()*
Computes the gradient of current tensor w.r.t. graph leaves. To iterate it backward ``PyTorch``does this with the code ``backward``but you can also do it yourself and iterate over ``net.parameters()`` and distribute it how you want it.

- *optimizer.step()*

  Performs a single optimization step (parameter update).It will adjust the weights for us.

Everything is still running on the CPU and that is why they are slow. We will move all to the GPU in the next steps.

"""

Epochs=3
for epoch in range(Epochs):
  for data in trainset:
    X, y= data # Grab the features (X) and labels (y) from current batch
    #print(X[0])
    #print(y[0])
    #break
    net.zero_grad() # Zero the gradients
    output= net(X.view(-1, 28*28)) # pass in the reshaped batch through the network
    loss=F.nll_loss(output, y) # calc and grab the loss value
    loss.backward() # apply this loss backwards thru the network's parameters
    optimizer.step() # attempt to optimize weights to account for loss/gradients
  print(loss) # print loss. We hope loss (a measure of wrong-ness) declines!

"""## Accuracy

We did calculate the loss but here we are focusing on the accuracy. All we need to do is iterate over our test set, measuring for correctness by comparing output to target values.

- *torch.no_grad()*

  Context-manager that disabled gradient calculation. Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward().
We are trying to validate our data. We don't want gradients to be calculated since we don't want to optimize based on this data.

### Comparison

We are comparing every prediction made with the actual target value and if it is correct we add one to the correct.

Be carefull because it is very easy to mess the neural network with some bias that you are adding without realising. The accuracy might get very high but not a good way!
"""

correct = 0
total = 0

with torch.no_grad():
  #Batch of information
  for data in testset:
    X, y= data
    output = net(X.view(-1, 784))
    #Comparing
    for idx, i in enumerate(output):
      if torch.argmax(i)== y[idx]:
        correct +=1
      total +=1
print("Accuracy: ", round(correct/total, 3))

"""## Plotting

Change the elements of X and see it for different result.
"""

import matplotlib.pyplot as plt

plt.imshow(X[1].view(28,28)) #getting it back to the image form
plt.show()

"""Since we are inputting a list we will get a list back and we need to say the zeroeth element."""

print(torch.argmax(net(X[1].view(-1,784))[0]))